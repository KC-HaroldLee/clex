{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compose()는 이미지 데이터 전처리 증강등의 과정에서 사용됨\n",
    "\n",
    "'''\n",
    "data_transform ={\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize([64,64]),\n",
    "        transforms.RandomHorizontalFlip(p=0.5), # p=반전되는 비율\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomCrop(52),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229,0.224,0.225])\n",
    "    ]),\n",
    "    'val' : transforms.Compose([\n",
    "        transforms.Resize([64,64]),\n",
    "        transforms.RandomCrop(52),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229,0.224,0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = '../0_data'\n",
    "\n",
    "image_datasets = {x : ImageFolder(root=os.path.join(data_dir, x), \n",
    "            transform=data_transform[x]) for x in ['train', 'val']}\n",
    "\n",
    "dataloaders = {x : torch.utils.data.DataLoader(image_datasets[x], \n",
    "            batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x : len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained Model 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models # 왜? 여기에서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet = models.resnet50(pretrained=True)\n",
    "effb1= models.efficientnet_b1(pretrained=True)\n",
    "num_ftrs = effb1.classifier[1].in_features # 이렇게까지 찾아야하나...\n",
    "effb1.classifier[1] = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "effb1 = effb1.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(filter(lambda p : p.requires_grad, \n",
    "                        effb1.parameters()), lr=0.001)\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 만약 freeze를 해야겠다면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for child in effb1.children():\n",
    "    cnt += 1\n",
    "    if cnt < 0:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eff(model, criterion, optimizer, scheduler, num_epochs):\n",
    "\n",
    "    best_model = copy.deepcopy(model.state_dict()) # 베스트모델\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'--------epoch {epoch+1}--------')\n",
    "        since = time.time()\n",
    "\n",
    "        for phase in ['train', 'val']: # 페이즈 순서\n",
    "            if phase == 'train':\n",
    "                model.train() # 페이즈에 맞게 모드 설정 | 기억은 안 나지만 가중치 변경이 가능하도록\n",
    "            elif phase == 'val' :\n",
    "                model.eval() # 페이즈에 맞게 모드 설정 | 기억은 안 나지만 가중치 고정\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]: # 두개를 동시에 뱉는다!\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'): # 모델의 Gradient를 업데이트\n",
    "                    outputs = model(inputs) # 아웃풋... pred는 더 뒤에\n",
    "                    _, preds = torch.max(outputs, 1) # 가장 높은 것을 가져온다.\n",
    "                    loss = criterion(outputs, labels) # pred와 gt의 사이 값을 구한다.\n",
    "\n",
    "                    if phase == 'train': # 파라미터 업데이트 구간이라는데...\n",
    "                        loss.backward()\n",
    "                        optimizer.step() \n",
    "            \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                l_r = [x['lr'] for x in optimizer_ft.param_groups] # 7에프크 마다 lr을 수정한다.\n",
    "                print('learning rate :', l_r)\n",
    "\n",
    "            epoch_loss = running_loss/dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double()/dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'\n",
    "                                .format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc :\n",
    "                best_acc = epoch_acc\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        \n",
    "        time_took = time.time() - since\n",
    "        print('Completed in {:.0f}m, {:.0f}s'\n",
    "                            .format(time_took//60, time_took%60))\n",
    "        \n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------epoch 1--------\n",
      "learning rate : [0.001]\n",
      "train Loss: 0.7270 Acc: 0.8034\n",
      "val Loss: 0.1609 Acc: 0.9479\n",
      "Completed in 0m, 45s\n",
      "--------epoch 2--------\n",
      "learning rate : [0.001]\n",
      "train Loss: 0.1375 Acc: 0.9558\n",
      "val Loss: 0.1036 Acc: 0.9665\n",
      "Completed in 0m, 33s\n",
      "--------epoch 3--------\n",
      "learning rate : [0.001]\n",
      "train Loss: 0.0896 Acc: 0.9710\n",
      "val Loss: 0.0649 Acc: 0.9783\n",
      "Completed in 0m, 34s\n",
      "--------epoch 4--------\n",
      "learning rate : [0.001]\n",
      "train Loss: 0.0648 Acc: 0.9778\n",
      "val Loss: 0.0839 Acc: 0.9729\n",
      "Completed in 0m, 34s\n",
      "--------epoch 5--------\n",
      "learning rate : [0.001]\n",
      "train Loss: 0.0605 Acc: 0.9798\n",
      "val Loss: 0.0894 Acc: 0.9697\n",
      "Completed in 0m, 34s\n",
      "--------epoch 6--------\n",
      "learning rate : [0.001]\n",
      "train Loss: 0.0527 Acc: 0.9829\n",
      "val Loss: 0.0624 Acc: 0.9805\n",
      "Completed in 0m, 34s\n",
      "--------epoch 7--------\n",
      "learning rate : [0.0001]\n",
      "train Loss: 0.0440 Acc: 0.9856\n",
      "val Loss: 0.0397 Acc: 0.9871\n",
      "Completed in 0m, 34s\n",
      "--------epoch 8--------\n",
      "learning rate : [0.0001]\n",
      "train Loss: 0.0203 Acc: 0.9929\n",
      "val Loss: 0.0290 Acc: 0.9916\n",
      "Completed in 0m, 35s\n",
      "--------epoch 9--------\n",
      "learning rate : [0.0001]\n",
      "train Loss: 0.0146 Acc: 0.9953\n",
      "val Loss: 0.0227 Acc: 0.9929\n",
      "Completed in 0m, 35s\n",
      "--------epoch 10--------\n",
      "learning rate : [0.0001]\n",
      "train Loss: 0.0122 Acc: 0.9959\n",
      "val Loss: 0.0236 Acc: 0.9922\n",
      "Completed in 0m, 35s\n",
      "--------epoch 11--------\n",
      "learning rate : [0.0001]\n",
      "train Loss: 0.0109 Acc: 0.9966\n",
      "val Loss: 0.0215 Acc: 0.9924\n",
      "Completed in 0m, 35s\n",
      "--------epoch 12--------\n",
      "learning rate : [0.0001]\n",
      "train Loss: 0.0095 Acc: 0.9973\n",
      "val Loss: 0.0207 Acc: 0.9933\n",
      "Completed in 0m, 35s\n",
      "--------epoch 13--------\n",
      "learning rate : [0.0001]\n",
      "train Loss: 0.0091 Acc: 0.9971\n",
      "val Loss: 0.0196 Acc: 0.9930\n",
      "Completed in 0m, 35s\n",
      "--------epoch 14--------\n",
      "learning rate : [1e-05]\n",
      "train Loss: 0.0084 Acc: 0.9974\n",
      "val Loss: 0.0182 Acc: 0.9934\n",
      "Completed in 0m, 35s\n",
      "--------epoch 15--------\n",
      "learning rate : [1e-05]\n",
      "train Loss: 0.0077 Acc: 0.9978\n",
      "val Loss: 0.0192 Acc: 0.9940\n",
      "Completed in 0m, 35s\n",
      "--------epoch 16--------\n",
      "learning rate : [1e-05]\n",
      "train Loss: 0.0075 Acc: 0.9976\n",
      "val Loss: 0.0170 Acc: 0.9946\n",
      "Completed in 0m, 35s\n",
      "--------epoch 17--------\n",
      "learning rate : [1e-05]\n",
      "train Loss: 0.0069 Acc: 0.9981\n",
      "val Loss: 0.0178 Acc: 0.9942\n",
      "Completed in 0m, 36s\n",
      "--------epoch 18--------\n",
      "learning rate : [1e-05]\n",
      "train Loss: 0.0061 Acc: 0.9981\n",
      "val Loss: 0.0188 Acc: 0.9934\n",
      "Completed in 0m, 36s\n",
      "--------epoch 19--------\n",
      "learning rate : [1e-05]\n",
      "train Loss: 0.0072 Acc: 0.9980\n",
      "val Loss: 0.0176 Acc: 0.9944\n",
      "Completed in 0m, 34s\n",
      "--------epoch 20--------\n",
      "learning rate : [1e-05]\n",
      "train Loss: 0.0064 Acc: 0.9979\n",
      "val Loss: 0.0202 Acc: 0.9939\n",
      "Completed in 0m, 35s\n",
      "--------epoch 21--------\n",
      "learning rate : [1.0000000000000002e-06]\n",
      "train Loss: 0.0061 Acc: 0.9979\n",
      "val Loss: 0.0188 Acc: 0.9935\n",
      "Completed in 0m, 34s\n",
      "--------epoch 22--------\n",
      "learning rate : [1.0000000000000002e-06]\n",
      "train Loss: 0.0077 Acc: 0.9974\n",
      "val Loss: 0.0202 Acc: 0.9934\n",
      "Completed in 0m, 35s\n",
      "--------epoch 23--------\n",
      "learning rate : [1.0000000000000002e-06]\n",
      "train Loss: 0.0071 Acc: 0.9978\n",
      "val Loss: 0.0189 Acc: 0.9937\n",
      "Completed in 0m, 34s\n",
      "--------epoch 24--------\n",
      "learning rate : [1.0000000000000002e-06]\n",
      "train Loss: 0.0074 Acc: 0.9973\n",
      "val Loss: 0.0174 Acc: 0.9935\n",
      "Completed in 0m, 33s\n",
      "--------epoch 25--------\n",
      "learning rate : [1.0000000000000002e-06]\n",
      "train Loss: 0.0059 Acc: 0.9980\n",
      "val Loss: 0.0201 Acc: 0.9936\n",
      "Completed in 0m, 35s\n",
      "--------epoch 26--------\n",
      "learning rate : [1.0000000000000002e-06]\n",
      "train Loss: 0.0064 Acc: 0.9978\n",
      "val Loss: 0.0191 Acc: 0.9936\n",
      "Completed in 0m, 35s\n",
      "--------epoch 27--------\n",
      "learning rate : [1.0000000000000002e-06]\n",
      "train Loss: 0.0060 Acc: 0.9981\n",
      "val Loss: 0.0217 Acc: 0.9927\n",
      "Completed in 0m, 35s\n",
      "--------epoch 28--------\n",
      "learning rate : [1.0000000000000002e-07]\n",
      "train Loss: 0.0065 Acc: 0.9978\n",
      "val Loss: 0.0200 Acc: 0.9937\n",
      "Completed in 0m, 35s\n",
      "--------epoch 29--------\n",
      "learning rate : [1.0000000000000002e-07]\n",
      "train Loss: 0.0066 Acc: 0.9978\n",
      "val Loss: 0.0169 Acc: 0.9940\n",
      "Completed in 0m, 35s\n",
      "--------epoch 30--------\n",
      "learning rate : [1.0000000000000002e-07]\n",
      "train Loss: 0.0069 Acc: 0.9980\n",
      "val Loss: 0.0169 Acc: 0.9940\n",
      "Completed in 0m, 36s\n",
      "Best val Acc: 0.994583\n"
     ]
    }
   ],
   "source": [
    "model_effb1 = train_eff(effb1, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_effb1, 'effb1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()  \n",
    "    test_loss = 0 \n",
    "    correct = 0   \n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for data, target in test_loader:  \n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)  \n",
    "            output = model(data) \n",
    "            \n",
    "            test_loss += F.cross_entropy(output,target, reduction='sum').item() \n",
    " \n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() \n",
    "   \n",
    "    test_loss /= len(test_loader.dataset) \n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset) \n",
    "    return test_loss, test_accuracy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_effb1 = transforms.Compose([\n",
    "        transforms.Resize([64,64]),  \n",
    "        transforms.RandomCrop(52),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_effb1 = ImageFolder(root='../0_data/test', transform=transform_effb1) \n",
    "test_loader_effb1 = torch.utils.data.DataLoader(test_effb1, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effb1 test acc:   99.32292136860161\n"
     ]
    }
   ],
   "source": [
    "resnet50=torch.load('effb1.pt') \n",
    "resnet50.eval()  \n",
    "test_loss, test_accuracy = evaluate(resnet50, test_loader_effb1)\n",
    "\n",
    "print('effb1 test acc:  ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9df3245e3a06411960d11e82b6a27359dd4d90689f0c5704b1aca31d06da11a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
